{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8w6YPejI8bii",
        "1qpPLv797xly",
        "Fy1teidG8kFy"
      ],
      "authorship_tag": "ABX9TyMZMjTLsuyIwHf0JhfsFbvM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/The-Talking-Head/Source-code-and-Dataset/blob/master/Source%20Code/5_fold_Split_of_Grocery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I have run this code on my pc in a local IDE, I have not run this code on google colab. This is given here for convenience**"
      ],
      "metadata": {
        "id": "A39ctWM7QAQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The code for K-split\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "\n",
        "# Setting the path to dataset folders\n",
        "base_folder = \"D:/Grocery_master\"\n",
        "\n",
        "# Setting the desired image size for preprocessing\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Loading the images and labels from the train, test, and validation folders using data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_folder = os.path.join(base_folder, 'train')\n",
        "train_generator = train_datagen.flow_from_directory(train_folder, target_size=image_size, batch_size=4, class_mode='categorical', color_mode='rgb')\n",
        "\n",
        "test_folder = os.path.join(base_folder, 'test')\n",
        "test_generator = test_datagen.flow_from_directory(test_folder, target_size=image_size, batch_size=4, class_mode='categorical', color_mode='rgb')\n",
        "\n",
        "\n",
        "validation_folder = os.path.join(base_folder, 'validation')\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_folder, target_size=image_size, batch_size=4, class_mode='categorical', color_mode='rgb')\n",
        "\n",
        "# Getting the number of classes\n",
        "num_classes = train_generator.num_classes\n",
        "\n",
        "# Preprocessing the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_generator.classes)\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "\n",
        "# Loading the base model from your device\n",
        "base_model_path = \"D:/GroceryKsplit/ensemble_model_vgg_mob\"\n",
        "base_model = load_model(base_model_path)\n",
        "\n",
        "# Creating a new model and add the base model layers\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Setting the number of folds for cross-validation\n",
        "k = 5\n",
        "\n",
        "# Performing K-fold cross-validation\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# Initializing lists to store the evaluation metrics\n",
        "accuracy_scores = []\n",
        "loss_scores = []\n",
        "models = []\n",
        "\n",
        "# Iterating over the K folds\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_generator.filenames), 1):\n",
        "    # Filtering the train and validation indices to avoid out-of-index problem\n",
        "    train_index = train_index[train_index < len(train_generator)]\n",
        "    val_index = val_index[val_index < len(train_generator)]\n",
        "\n",
        "    # Getting the current fold data generators\n",
        "    train_data_generator = train_datagen.flow_from_directory(\n",
        "        train_folder,\n",
        "        target_size=image_size,\n",
        "        batch_size=4,\n",
        "        class_mode='categorical',\n",
        "        color_mode='rgb',\n",
        "        subset='training',\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "\n",
        "    val_data_generator = validation_datagen.flow_from_directory(\n",
        "        validation_folder,\n",
        "        target_size=image_size,\n",
        "        batch_size=4,\n",
        "        class_mode='categorical',\n",
        "        color_mode='rgb',\n",
        "        shuffle=False\n",
        "    )\n",
        "    # Training the model on the current fold\n",
        "    model.fit(train_data_generator, epochs=10, verbose=1)\n",
        "\n",
        "    # Evaluating the model on the validation set of the current fold\n",
        "    loss, accuracy = model.evaluate(val_data_generator)\n",
        "\n",
        "    # Storing the evaluation metrics for the current fold\n",
        "    accuracy_scores.append(accuracy)\n",
        "    loss_scores.append(loss)\n",
        "\n",
        "    # Saving the model for the current fold\n",
        "    model.save(f\"model_fold_{fold}.h5\")\n",
        "    models.append(f\"model_fold_{fold}.h5\")\n",
        "\n",
        "# Calculating and printing the average evaluation metrics across all folds\n",
        "avg_accuracy = np.mean(accuracy_scores)\n",
        "avg_loss = np.mean(loss_scores)\n",
        "print(\"Average Accuracy:\", avg_accuracy)\n",
        "print(\"Average Loss:\", avg_loss)"
      ],
      "metadata": {
        "id": "GgWfRflPCObR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results found for the code**"
      ],
      "metadata": {
        "id": "8w6YPejI8bii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "D:\\GroceryKsplit\\venv\\Scripts\\python.exe D:\\GroceryKsplit\\main2.py\n",
        "Found 2509 images belonging to 3 classes.\n",
        "Found 356 images belonging to 3 classes.\n",
        "Found 297 images belonging to 3 classes.\n",
        "Found 2509 images belonging to 3 classes.\n",
        "Found 297 images belonging to 3 classes.\n",
        "Epoch 1/10\n",
        "628/628 [==============================] - 707s 1s/step - loss: 0.3337 - accuracy: 0.8888\n",
        "Epoch 2/10\n",
        "628/628 [==============================] - 693s 1s/step - loss: 0.1908 - accuracy: 0.9195\n",
        "Epoch 3/10\n",
        "628/628 [==============================] - 694s 1s/step - loss: 0.1996 - accuracy: 0.9187\n",
        "Epoch 4/10\n",
        "628/628 [==============================] - 695s 1s/step - loss: 0.1584 - accuracy: 0.9426\n",
        "Epoch 5/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.1406 - accuracy: 0.9466\n",
        "Epoch 6/10\n",
        "628/628 [==============================] - 693s 1s/step - loss: 0.1395 - accuracy: 0.9446\n",
        "Epoch 7/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.1271 - accuracy: 0.9490\n",
        "Epoch 8/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.1054 - accuracy: 0.9578\n",
        "Epoch 9/10\n",
        "628/628 [==============================] - 696s 1s/step - loss: 0.1220 - accuracy: 0.9502\n",
        "Epoch 10/10\n",
        "628/628 [==============================] - 723s 1s/step - loss: 0.1169 - accuracy: 0.9562\n",
        "75/75 [==============================] - 84s 1s/step - loss: 0.1227 - accuracy: 0.9394\n",
        "Found 2509 images belonging to 3 classes.\n",
        "Found 297 images belonging to 3 classes.\n",
        "Epoch 1/10\n",
        "628/628 [==============================] - 713s 1s/step - loss: 0.1103 - accuracy: 0.9617\n",
        "Epoch 2/10\n",
        "628/628 [==============================] - 712s 1s/step - loss: 0.0993 - accuracy: 0.9649\n",
        "Epoch 3/10\n",
        "628/628 [==============================] - 716s 1s/step - loss: 0.1035 - accuracy: 0.9625\n",
        "Epoch 4/10\n",
        "628/628 [==============================] - 704s 1s/step - loss: 0.1052 - accuracy: 0.9641\n",
        "Epoch 5/10\n",
        "628/628 [==============================] - 706s 1s/step - loss: 0.0950 - accuracy: 0.9701\n",
        "Epoch 6/10\n",
        "628/628 [==============================] - 695s 1s/step - loss: 0.1045 - accuracy: 0.9649\n",
        "Epoch 7/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0885 - accuracy: 0.9697\n",
        "Epoch 8/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0910 - accuracy: 0.9721\n",
        "Epoch 9/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0804 - accuracy: 0.9765\n",
        "Epoch 10/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0909 - accuracy: 0.9721\n",
        "75/75 [==============================] - 82s 1s/step - loss: 0.1038 - accuracy: 0.9663\n",
        "Found 2509 images belonging to 3 classes.\n",
        "Found 297 images belonging to 3 classes.\n",
        "Epoch 1/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.0747 - accuracy: 0.9777\n",
        "Epoch 2/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.0743 - accuracy: 0.9753\n",
        "Epoch 3/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0801 - accuracy: 0.9725\n",
        "Epoch 4/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0814 - accuracy: 0.9761\n",
        "Epoch 5/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0711 - accuracy: 0.9777\n",
        "Epoch 6/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0748 - accuracy: 0.9805\n",
        "Epoch 7/10\n",
        "628/628 [==============================] - 694s 1s/step - loss: 0.0702 - accuracy: 0.9789\n",
        "Epoch 8/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0630 - accuracy: 0.9793\n",
        "Epoch 9/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0708 - accuracy: 0.9785\n",
        "Epoch 10/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0687 - accuracy: 0.9781\n",
        "75/75 [==============================] - 81s 1s/step - loss: 0.0849 - accuracy: 0.9663\n",
        "Found 2509 images belonging to 3 classes.\n",
        "Found 297 images belonging to 3 classes.\n",
        "Epoch 1/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0561 - accuracy: 0.9849\n",
        "Epoch 2/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0650 - accuracy: 0.9817\n",
        "Epoch 3/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0606 - accuracy: 0.9821\n",
        "Epoch 4/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0624 - accuracy: 0.9837\n",
        "Epoch 5/10\n",
        "628/628 [==============================] - 695s 1s/step - loss: 0.0595 - accuracy: 0.9833\n",
        "Epoch 6/10\n",
        "628/628 [==============================] - 699s 1s/step - loss: 0.0555 - accuracy: 0.9868\n",
        "Epoch 7/10\n",
        "628/628 [==============================] - 698s 1s/step - loss: 0.0588 - accuracy: 0.9809\n",
        "Epoch 8/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.0536 - accuracy: 0.9857\n",
        "Epoch 9/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0470 - accuracy: 0.9864\n",
        "Epoch 10/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0416 - accuracy: 0.9880\n",
        "75/75 [==============================] - 82s 1s/step - loss: 0.1693 - accuracy: 0.9596\n",
        "Found 2509 images belonging to 3 classes.\n",
        "Found 297 images belonging to 3 classes.\n",
        "Epoch 1/10\n",
        "628/628 [==============================] - 691s 1s/step - loss: 0.0439 - accuracy: 0.9892\n",
        "Epoch 2/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.0355 - accuracy: 0.9916\n",
        "Epoch 3/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.0452 - accuracy: 0.9868\n",
        "Epoch 4/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0392 - accuracy: 0.9912\n",
        "Epoch 5/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0425 - accuracy: 0.9892\n",
        "Epoch 6/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0489 - accuracy: 0.9841\n",
        "Epoch 7/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.0394 - accuracy: 0.9892\n",
        "Epoch 8/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.0432 - accuracy: 0.9900\n",
        "Epoch 9/10\n",
        "628/628 [==============================] - 692s 1s/step - loss: 0.0319 - accuracy: 0.9924\n",
        "Epoch 10/10\n",
        "628/628 [==============================] - 690s 1s/step - loss: 0.0411 - accuracy: 0.9904\n",
        "75/75 [==============================] - 82s 1s/step - loss: 0.1376 - accuracy: 0.9731\n",
        "Average Accuracy: 0.9609427690505982\n",
        "Average Loss: 0.12364481687545777\n",
        "\n",
        "Process finished with exit code 0"
      ],
      "metadata": {
        "id": "6mlQaHUoCRVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing the saved 5 models and calculating their accuracy, precision, recall and f1 score.**"
      ],
      "metadata": {
        "id": "1qpPLv797xly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to your dataset folders\n",
        "base_folder = \"/content/drive/MyDrive/Grocery\"\n",
        "validation_folder = os.path.join(base_folder, 'validation')\n",
        "\n",
        "# Set the desired image size for preprocessing\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Load the images and labels from the validation folder using a data generator\n",
        "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_folder, target_size=image_size, batch_size=4, class_mode='categorical', color_mode='rgb', shuffle=False)\n",
        "\n",
        "# Set the path to the folder containing the saved models\n",
        "models_folder = \"/content/drive/MyDrive/Grocery/Split result models\"\n",
        "\n",
        "# Initialize lists to store evaluation metrics\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "# Load and evaluate the models\n",
        "for i in range(1, 6):\n",
        "    model_path = os.path.join(models_folder, f\"model_fold_{i}.h5\")\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    loss, accuracy = model.evaluate(validation_generator, verbose=0)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Predict labels for the validation set\n",
        "    y_pred = model.predict(validation_generator)\n",
        "    y_true = validation_generator.classes\n",
        "\n",
        "    # Convert predicted probabilities to class labels\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = precision_score(y_true, y_pred_labels, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred_labels, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred_labels, average='weighted')\n",
        "\n",
        "    # Append the metrics to the respective lists\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Print the metrics for the current fold\n",
        "    print(f\"Model {i}:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1-score: {f1}\")\n",
        "    print(\"\")\n",
        "\n",
        "# Calculate the average metrics across all folds\n",
        "avg_accuracy = np.mean(accuracies)\n",
        "avg_precision = np.mean(precisions)\n",
        "avg_recall = np.mean(recalls)\n",
        "avg_f1_score = np.mean(f1_scores)\n",
        "\n",
        "# Print the average metrics\n",
        "print(\"Average Metrics:\")\n",
        "print(f\"Accuracy: {avg_accuracy}\")\n",
        "print(f\"Precision: {avg_precision}\")\n",
        "print(f\"Recall: {avg_recall}\")\n",
        "print(f\"F1-score: {avg_f1_score}\")\n"
      ],
      "metadata": {
        "id": "9KCujAthxj9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results for the accuracy, prec, recall, f1 score**"
      ],
      "metadata": {
        "id": "Fy1teidG8kFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Found 297 images belonging to 3 classes.\n",
        "75/75 [==============================] - 177s 2s/step\n",
        "Model 1:\n",
        "Accuracy: 0.939393937587738\n",
        "Precision: 0.9412497892921512\n",
        "Recall: 0.9393939393939394\n",
        "F1-score: 0.9376728378932705\n",
        "\n",
        "75/75 [==============================] - 176s 2s/step\n",
        "Model 2:\n",
        "Accuracy: 0.9629629850387573\n",
        "Precision: 0.9629111852451893\n",
        "Recall: 0.9629629629629629\n",
        "F1-score: 0.9628160654658908\n",
        "\n",
        "75/75 [==============================] - 176s 2s/step\n",
        "Model 3:\n",
        "Accuracy: 0.9696969985961914\n",
        "Precision: 0.969666403992645\n",
        "Recall: 0.9696969696969697\n",
        "F1-score: 0.9696352510278974\n",
        "\n",
        "75/75 [==============================] - 180s 2s/step\n",
        "Model 4:\n",
        "Accuracy: 0.9629629850387573\n",
        "Precision: 0.9637142399047162\n",
        "Recall: 0.9629629629629629\n",
        "F1-score: 0.9623499197898854\n",
        "\n",
        "75/75 [==============================] - 179s 2s/step\n",
        "Model 5:\n",
        "Accuracy: 0.9764309525489807\n",
        "Precision: 0.9766270285750804\n",
        "Recall: 0.9764309764309764\n",
        "F1-score: 0.9763404687852701\n",
        "\n",
        "Average Metrics:\n",
        "Accuracy: 0.962289571762085\n",
        "Precision: 0.9628337294019564\n",
        "Recall: 0.9622895622895623\n",
        "F1-score: 0.9617629085924427"
      ],
      "metadata": {
        "id": "zv-CpVfaydLs"
      }
    }
  ]
}